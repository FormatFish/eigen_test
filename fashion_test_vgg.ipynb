{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import mnist_reader\n",
    "import numpy as np\n",
    "import tflearn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape , X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f75f07e6190>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARoAAAEYCAYAAACDezmxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7680ac6490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "io.imshow(X_train[1666].reshape(-1 , 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tflearn.layers.core import input_data, dropout, fully_connected , one_hot_encoding\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/envs/tensor/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<type 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  1,  1, ...,  4,  1,  6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.n_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape([-1 , 28 , 28 , 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building 'VGG Network'\n",
    "network = input_data(shape=[None, 28 , 28 , 1])\n",
    "\n",
    "network = conv_2d(network, 64, 3, activation='relu')\n",
    "network = conv_2d(network, 64, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "network = conv_2d(network, 128, 3, activation='relu')\n",
    "network = conv_2d(network, 128, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "network = conv_2d(network, 256, 3, activation='relu')\n",
    "network = conv_2d(network, 256, 3, activation='relu')\n",
    "network = conv_2d(network, 256, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "network = conv_2d(network, 512, 3, activation='relu')\n",
    "network = conv_2d(network, 512, 3, activation='relu')\n",
    "network = conv_2d(network, 512, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "network = conv_2d(network, 512, 3, activation='relu')\n",
    "network = conv_2d(network, 512, 3, activation='relu')\n",
    "network = conv_2d(network, 512, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "network = fully_connected(network, 4096, activation='relu')\n",
    "network = dropout(network, 0.5)\n",
    "network = fully_connected(network, 4096, activation='relu')\n",
    "network = dropout(network, 0.5)\n",
    "network = fully_connected(network, 10, activation='softmax')\n",
    "\n",
    "network = regression(network, optimizer='rmsprop',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     to_one_hot = True , \n",
    "                     n_classes = 10 , \n",
    "                     learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tflearn.DNN(network, checkpoint_path='./models/model_vgg',\n",
    "                    max_checkpoints=1, tensorboard_verbose=2 , tensorboard_dir = \"./logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 14999  | total loss: \u001b[1m\u001b[32m0.18761\u001b[0m\u001b[0m | time: 1159.911s\n",
      "| RMSProp | epoch: 010 | loss: 0.18761 - acc: 0.9382 -- iter: 59960/60000\n",
      "Training Step: 15000  | total loss: \u001b[1m\u001b[32m0.19531\u001b[0m\u001b[0m | time: 1160.681s\n",
      "| RMSProp | epoch: 010 | loss: 0.19531 - acc: 0.9319 -- iter: 60000/60000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, n_epoch=10, shuffle=True,\n",
    "          show_metric=True, batch_size=40, snapshot_step=500,\n",
    "          snapshot_epoch=False, run_id='vgg_oxflowers17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /root/fashion-mnist/models/fashion_mnist.tflearn\n"
     ]
    }
   ],
   "source": [
    "model.load(\"./models/fashion_mnist.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = X_test.reshape([-1 , 28 , 28 , 1])\n",
    "y_pred = model.predict(X_test[:8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "y_pred_alex = pickle.load(open(\"y_pred.dat\" , \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, ..., 8, 1, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, ..., 8, 1, 5], dtype=uint8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function argmax in module numpy.core.fromnumeric:\n",
      "\n",
      "argmax(a, axis=None, out=None)\n",
      "    Returns the indices of the maximum values along an axis.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Input array.\n",
      "    axis : int, optional\n",
      "        By default, the index is into the flattened array, otherwise\n",
      "        along the specified axis.\n",
      "    out : array, optional\n",
      "        If provided, the result will be inserted into this array. It should\n",
      "        be of the appropriate shape and dtype.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    index_array : ndarray of ints\n",
      "        Array of indices into the array. It has the same shape as `a.shape`\n",
      "        with the dimension along `axis` removed.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    ndarray.argmax, argmin\n",
      "    amax : The maximum value along a given axis.\n",
      "    unravel_index : Convert a flat index into an index tuple.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    In case of multiple occurrences of the maximum values, the indices\n",
      "    corresponding to the first occurrence are returned.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.arange(6).reshape(2,3)\n",
      "    >>> a\n",
      "    array([[0, 1, 2],\n",
      "           [3, 4, 5]])\n",
      "    >>> np.argmax(a)\n",
      "    5\n",
      "    >>> np.argmax(a, axis=0)\n",
      "    array([1, 1, 1])\n",
      "    >>> np.argmax(a, axis=1)\n",
      "    array([2, 2])\n",
      "    \n",
      "    >>> b = np.arange(6)\n",
      "    >>> b[1] = 5\n",
      "    >>> b\n",
      "    array([0, 5, 2, 3, 4, 5])\n",
      "    >>> np.argmax(b) # Only the first occurrence is returned.\n",
      "    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90749999999999997"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.argmax(y_pred , axis=1) == y_test[:8000])/8000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.877"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.argmax(y_pred_alex , axis=1) == y_test[:8000])/8000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_comb =( y_pred + y_pred_alex) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91100000000000003"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.argmax(y_comb , axis=1) == y_test[:8000])/8000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensor)",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
